{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ACHAL GANDHI\n",
        "# 218319111\n",
        "# Final math 4171"
      ],
      "metadata": {
        "id": "1Pr-P8usZOPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question (1)**\n",
        "## **Implementations and Results of Nelder-Meader, Steepdest Descent, DFP, and BFGS**\n",
        "## **Table is created at end of question 1**"
      ],
      "metadata": {
        "id": "h6KVaLqJOhg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T8FoyJUBx4hq"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.optimize as opt\n",
        "import matplotlib.pyplot as plt\n",
        "from prettytable import PrettyTable\n",
        "import warnings\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining powell_bs, brown_den, gulf function**"
      ],
      "metadata": {
        "id": "aedvBz85FqI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def powell_bs(par):\n",
        "  par=np.append(0,par)\n",
        "  s=len(par)\n",
        "  h=np.zeros((s,s))\n",
        "  x = par[1]\n",
        "  y = par[2]\n",
        "\n",
        "  f1 = 1e4 * x * y - 1\n",
        "  f2 = np.exp(-x) + np.exp(-y) - 1.0001\n",
        "\n",
        "  a = 2e4 * f1\n",
        "  b = 2 * f2\n",
        "\n",
        "\n",
        "  fn = f1 * f1 + f2 * f2\n",
        "  gr = np.array([y * a - np.exp(-x) * b, x * a - np.exp(-y) * b])\n",
        "\n",
        "  x1 = par[1]\n",
        "  x2 = par[2]\n",
        "\n",
        "  t1 = 1.0e+04*x1*x2 - 1\n",
        "  t2 = np.exp( - x1 ) + np.exp( - x2 ) - 1.0001\n",
        "  h[1,1] = 2.0*( 1.0e+8*x2 ** 2 + t2*np.exp( - x1 ) + np.exp( - x1 ) ** 2 )\n",
        "  h[1,2] = 2.0*( t1*1.0e+04 + 1.0e+8*x1*x2 + np.exp( - x1 - x2 ) )\n",
        "  h[2,2] = 2.0*( 1.0e+8*x1 ** 2 + t2*np.exp( - x2 ) + np.exp( - x2 ) ** 2 )\n",
        "  h[2,1] = h[1,2]\n",
        "\n",
        "  h=np.delete(h,0,0)\n",
        "  h=np.delete(h,0,1)\n",
        "  return(np.array([fn,gr,h],dtype=object))\n",
        "\n",
        "x0 = np.array([1, 1])\n",
        "def brown_den(par):\n",
        "  par=np.append(0,par)\n",
        "  s=len(par)\n",
        "  h=np.zeros((s,s))\n",
        "  m=20\n",
        "  x1 = par[1]\n",
        "  x2 = par[2]\n",
        "  x3 = par[3]\n",
        "  x4 = par[4]\n",
        "\n",
        "  ti = np.linspace(1,m,num=m) * 0.2\n",
        "  sinti = np.sin(ti)\n",
        "  l = x1 + ti * x2 - np.exp(ti)\n",
        "  r = x3 + x4 * sinti - np.cos(ti)\n",
        "  f = l * l + r * r\n",
        "  lf4 = 4 * l * f\n",
        "  rf4 = 4 * r * f\n",
        "\n",
        "  fsum = sum(f * f)\n",
        "  grad = np.array([sum(lf4),sum(lf4 * ti),sum(rf4),sum(rf4 * sinti)])\n",
        "\n",
        "\n",
        "  for i in range(1,m+1):\n",
        "    d1 = i/5.0\n",
        "    d2 = np.sin( d1 )\n",
        "    t1 = x1 + d1*x2 - np.exp( d1 )\n",
        "    t2 = x3 + d2*x4 - np.cos( d1 )\n",
        "    t = 8.0 * t1 * t2\n",
        "    s1 = 12.0*t1 ** 2 + 4.0*t2 ** 2\n",
        "    s2 = 12.0*t2 ** 2 + 4.0*t1 ** 2\n",
        "    h[1,1] = h[1,1] + s1\n",
        "    h[1,2] = h[1,2] + s1*d1\n",
        "    h[2,2] = h[2,2] + s1*d1 ** 2\n",
        "    h[1,3] = h[1,3] + t\n",
        "    h[2,3] = h[2,3] + t*d1\n",
        "    h[3,3] = h[3,3] + s2\n",
        "    h[1,4] = h[1,4] + t*d2\n",
        "    h[2,4] = h[2,4] + t*d1*d2\n",
        "    h[3,4] = h[3,4] + s2*d2\n",
        "    h[4,4] = h[4,4] + s2*d2 ** 2\n",
        "\n",
        "  h[2,1] = h[1,2]\n",
        "  h[3,1] = h[1,3]\n",
        "  h[3,2] = h[2,3]\n",
        "  h[4,1] = h[1,4]\n",
        "  h[4,2] = h[2,4]\n",
        "  h[4,3] = h[3,4]\n",
        "\n",
        "\n",
        "  h=np.delete(h,0,0)\n",
        "  h=np.delete(h,0,1)\n",
        "  fn=fsum\n",
        "  gr=grad\n",
        "  return(np.array([fn,gr,h],dtype=object))\n",
        "\n",
        "a1 = np.array([25, 5, -5, 1])\n",
        "def gulf(par):\n",
        "  m=90\n",
        "  par=np.append(0,par)\n",
        "  s=len(par)\n",
        "  h=np.zeros((s,s))\n",
        "  p66 = 2 / 3\n",
        "\n",
        "  x1 = par[1]\n",
        "  x2 = par[2]\n",
        "  x3 = par[3]\n",
        "\n",
        "  ti = np.linspace(1,m,num=m) * 0.01\n",
        "  y = 25 + (-50 * np.log(ti)) ** p66\n",
        "\n",
        "  x2y = x2 - y\n",
        "  ax2y = abs(x2y)\n",
        "  x2yz = ax2y ** x3\n",
        "  e = np.exp(-x2yz / x1)\n",
        "  fi = e - ti\n",
        "  efxyz2 = 2 * e * fi * x2yz\n",
        "\n",
        "  dx = sum(efxyz2 / (x1 * x1))\n",
        "  dy = -sum(efxyz2 * x3 / (x1 * x2y))\n",
        "  dz = -sum(efxyz2 * np.log(ax2y) / x1)\n",
        "\n",
        "  fsum = sum(fi * fi)\n",
        "  grad = np.array([dx, dy, dz])\n",
        "\n",
        "  d1 = p66\n",
        "  for i in range(1,m+1):\n",
        "      arg = 0.01*i\n",
        "      r = ( -50.0*np.log( arg ) ) ** d1 + 25.0 - x2\n",
        "      t1 = abs( r ) ** x3/x1\n",
        "      t2 = np.exp( -t1 )\n",
        "      t3 = t1*t2*( t1*t2 + ( t1 - 1.0 )*( t2 - arg ) )\n",
        "      t = t1*t2*( t2 - arg )\n",
        "      logr = np.log( abs( r ) )\n",
        "      h[1,1] = h[1,1] + t3 - t\n",
        "      h[1,2] = h[1,2] + t3/r\n",
        "      h[2,2] = h[2,2] + ( t + x3*t3 )/r ** 2\n",
        "      h[1,3] = h[1,3] - t3*logr\n",
        "      h[2,3] = h[2,3] + (t-x3*t3*logr)/r\n",
        "      h[3,3] = h[3,3] + t3*logr ** 2\n",
        "\n",
        "  h[1,1] = h[1,1] / x1 ** 2\n",
        "  h[1,2] = h[1,2]*x3/x1\n",
        "  h[2,2] = h[2,2]*x3\n",
        "  h[1,3] = h[1,3] / x1\n",
        "  h = 2.0 * h\n",
        "  h[2,1] = h[1,2]\n",
        "  h[3,1] = h[1,3]\n",
        "  h[3,2] = h[2,3]\n",
        "\n",
        "  h=np.delete(h,0,0)\n",
        "  h=np.delete(h,0,1)\n",
        "  fn=fsum\n",
        "  gr=grad\n",
        "  return(np.array([fn,gr,h],dtype=object))\n",
        "\n",
        "a2 = np.array([5, 2.5, 0.15])"
      ],
      "metadata": {
        "id": "XjFmDvf6yGUJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wrapper functions for separating function, gradient, hessian**"
      ],
      "metadata": {
        "id": "0O3oFZbCF2z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pbo(par):\n",
        "  result = powell_bs(par)\n",
        "  return result[0]\n",
        "\n",
        "def bdo(par):\n",
        "  result = brown_den(par)\n",
        "  return result[0]\n",
        "\n",
        "def gulfo(par):\n",
        "  result = gulf(par)\n",
        "  return result[0]\n",
        "\n",
        "def pbg(par):\n",
        "  result = powell_bs(par)\n",
        "  return result[1]\n",
        "\n",
        "def bdg(par):\n",
        "  result = brown_den(par)\n",
        "  return result[1]\n",
        "\n",
        "def gulfg(par):\n",
        "  result = gulf(par)\n",
        "  return result[1]\n",
        "\n",
        "def pbh(par):\n",
        "  result = powell_bs(par)\n",
        "  return result[2]\n",
        "\n",
        "def bdh(par):\n",
        "  result = brown_den(par)\n",
        "  return result[2]\n",
        "\n",
        "def gulfh(par):\n",
        "  result = gulf(par)\n",
        "  return result[2]\n",
        "\n"
      ],
      "metadata": {
        "id": "lYzdnggCtxa5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nelder Mead Algorithm implementation and results.**"
      ],
      "metadata": {
        "id": "2XmBOn2jGHRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nelder_mead(obj_func, initial_guess, max_iter=1000, tol=1e-6, alpha=1, beta=0.5, gamma=2, delta=0.5):\n",
        "    \"\"\"\n",
        "    Perform optimization using the Nelder-Mead algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - obj_func: Objective function to be minimized.\n",
        "    - initial_guess: Starting point for the optimization.\n",
        "    - max_iter: Maximum number of iterations.\n",
        "    - tol: Tolerance for convergence.\n",
        "    - alpha: Reflection coefficient.\n",
        "    - beta: Contraction coefficient.\n",
        "    - gamma: Expansion coefficient.\n",
        "    - delta: Shrinkage coefficient.\n",
        "\n",
        "    Returns:\n",
        "    - The best point found.\n",
        "    - Number of iterations performed.\n",
        "    - Elapsed time.\n",
        "    - Boolean indicating if optimality was reached.\n",
        "    \"\"\"\n",
        "    n = len(initial_guess)\n",
        "    # Initialize a simplex around the initial guess\n",
        "    vertices = np.zeros((n + 1, n))\n",
        "    vertices[0] = initial_guess\n",
        "\n",
        "    # Create initial simplex\n",
        "    for i in range(n):\n",
        "        vertex = initial_guess.copy()\n",
        "        vertex[i] = initial_guess[i] + 1.0\n",
        "        vertices[i + 1] = vertex\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        # Evaluate the objective function for all vertices\n",
        "        values = np.array([obj_func(vertex) for vertex in vertices])\n",
        "\n",
        "        # Sort vertices by their function values\n",
        "        sorted_indices = np.argsort(values)\n",
        "        vertices = vertices[sorted_indices]\n",
        "\n",
        "        # Calculate centroid of all points except the worst\n",
        "        centroid = np.mean(vertices[:-1], axis=0)\n",
        "\n",
        "        # Reflection step\n",
        "        reflected_vertex = centroid + alpha * (centroid - vertices[-1])\n",
        "        reflected_value = obj_func(reflected_vertex)\n",
        "\n",
        "        if values[0] <= reflected_value < values[-2]:\n",
        "            # If the reflected point is better than the second worst, replace the worst point\n",
        "            vertices[-1] = reflected_vertex\n",
        "        elif reflected_value < values[0]:\n",
        "            # Expansion step if the reflected point is the best so far\n",
        "            expanded_vertex = centroid + gamma * (reflected_vertex - centroid)\n",
        "            expanded_value = obj_func(expanded_vertex)\n",
        "\n",
        "            # Choose between reflected and expanded point\n",
        "            if expanded_value < reflected_value:\n",
        "                vertices[-1] = expanded_vertex\n",
        "            else:\n",
        "                vertices[-1] = reflected_vertex\n",
        "        else:\n",
        "            # Contraction step\n",
        "            contracted_vertex = centroid + beta * (vertices[-1] - centroid)\n",
        "            contracted_value = obj_func(contracted_vertex)\n",
        "\n",
        "            if contracted_value < values[-1]:\n",
        "                vertices[-1] = contracted_vertex\n",
        "            else:\n",
        "                # Shrink the simplex towards the best point\n",
        "                for i in range(1, n + 1):\n",
        "                    vertices[i] = vertices[0] + delta * (vertices[i] - vertices[0])\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(vertices[0] - vertices[-1]) < tol:\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            return vertices[0], iteration + 1, elapsed_time, True\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Return the best point found, number of iterations, running time, and whether optimality was reached\n",
        "    return vertices[0], max_iter, elapsed_time, False\n",
        "\n",
        "# Suppress runtime warnings that may occur during optimization\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    result_powell_bs, iter_powell_bs, time_powell_bs, opt_powell_bs = nelder_mead(pbo, x0)\n",
        "    result_brown_den, iter_brown_den, time_brown_den, opt_brown_den = nelder_mead(bdo, a1)\n",
        "    result_gulf, iter_gulf, time_gulf, opt_gulf = nelder_mead(gulfo, a2)\n",
        "\n",
        "# Print results for the Nelder-Mead algorithm\n",
        "print(\"Results for Nelder-Mead algorithm\")\n",
        "print(\"Result for powell_bs:\", result_powell_bs)\n",
        "print(\"Number of iterations for powell_bs:\", iter_powell_bs)\n",
        "print(\"Running time for powell_bs:\", time_powell_bs)\n",
        "print(\"Optimality reached for powell_bs:\", opt_powell_bs)\n",
        "\n",
        "print(\"\\nResult for brown_den:\", result_brown_den)\n",
        "print(\"Number of iterations for brown_den:\", iter_brown_den)\n",
        "print(\"Running time for brown_den:\", time_brown_den)\n",
        "print(\"Optimality reached for brown_den:\", opt_brown_den)\n",
        "\n",
        "print(\"\\nResult for gulf:\", result_gulf)\n",
        "print(\"Number of iterations for gulf:\", iter_gulf)\n",
        "print(\"Running time for gulf:\", time_gulf)\n",
        "print(\"Optimality reached for gulf:\", opt_gulf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2CStrdSnk6C",
        "outputId": "8ccc8eba-3b5b-4240-8f91-f209a551a7de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Nelder-Mead algorithm\n",
            "Result for powell_bs: [9.10614509e+00 1.09815953e-05]\n",
            "Number of iterations for powell_bs: 509\n",
            "Running time for powell_bs: 1.2984683513641357\n",
            "Optimality reached for powell_bs: True\n",
            "\n",
            "Result for brown_den: [-11.59443956  13.20363     -0.40343998   0.23677878]\n",
            "Number of iterations for brown_den: 214\n",
            "Running time for brown_den: 1.726170539855957\n",
            "Optimality reached for brown_den: True\n",
            "\n",
            "Result for gulf: [50.00000249 24.99999994  1.50000002]\n",
            "Number of iterations for gulf: 274\n",
            "Running time for gulf: 7.42261815071106\n",
            "Optimality reached for gulf: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Steepdest Descent Algorithm implementation and results.**"
      ],
      "metadata": {
        "id": "voHrlzgiGRe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cubicmin(a, fa, fpa, b, fb, c, fc):\n",
        "    \"\"\"\n",
        "    Uses cubic interpolation to estimate the minimum of a function based on function and gradient\n",
        "    values at three points.\n",
        "    \"\"\"\n",
        "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
        "        try:\n",
        "            # Coefficients for cubic interpolation\n",
        "            C = fpa\n",
        "            db = b - a\n",
        "            dc = c - a\n",
        "            denom = (db * dc) ** 2 * (db - dc)\n",
        "            d1 = np.empty((2, 2))\n",
        "            d1[0, 0] = dc ** 2\n",
        "            d1[0, 1] = -db ** 2\n",
        "            d1[1, 0] = -dc ** 3\n",
        "            d1[1, 1] = db ** 3\n",
        "            # Linear system to solve for cubic coefficients A and B\n",
        "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db, fc - fa - C * dc]).flatten())\n",
        "            A /= denom\n",
        "            B /= denom\n",
        "            radical = B * B - 3 * A * C\n",
        "\n",
        "            # Discriminant must be non-negative; otherwise, take it as zero\n",
        "            radical = max(radical, 0)\n",
        "\n",
        "            # Minimizer for the cubic polynomial\n",
        "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
        "        except ArithmeticError:\n",
        "            # Handle any mathematical errors\n",
        "            return None\n",
        "    # Ensure the result is a finite number\n",
        "    if not np.isfinite(xmin):\n",
        "        return None\n",
        "    return xmin\n",
        "\n",
        "# Function to find the minimum of a quadratic polynomial\n",
        "def quadmin(a, fa, fpa, b, fb):\n",
        "    \"\"\"\n",
        "    Uses quadratic interpolation to estimate the minimum of a function based on function\n",
        "    values at two points and the gradient at one point.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        D, C, db = fa, fpa, b - a\n",
        "        B = (fb - D - C * db) / (db * db)\n",
        "        xmin = a - C / (2.0 * B)\n",
        "    except ArithmeticError:\n",
        "        # Handle any mathematical errors\n",
        "        return None\n",
        "    # Ensure the result is a finite number\n",
        "    return xmin if np.isfinite(xmin) else None\n",
        "\n",
        "# Helper function used in the line search to refine the step size\n",
        "def zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo, phi, derphi, phi0, derphi0, c1, c2):\n",
        "    \"\"\"\n",
        "    Part of the line search algorithm that zooms in on an interval where the Wolfe conditions\n",
        "    are likely to be satisfied.\n",
        "    \"\"\"\n",
        "    maxiter, i, delta1, delta2, phi_rec, a_rec = 100, 0, 0.2, 0.1, phi0, 0\n",
        "\n",
        "    while True:\n",
        "        dalpha = a_hi - a_lo\n",
        "        a, b = (a_hi, a_lo) if dalpha < 0 else (a_lo, a_hi)\n",
        "\n",
        "        # Use cubic interpolation if we have a previous value (phi_rec)\n",
        "        if (i > 0):\n",
        "            cchk = delta1 * dalpha\n",
        "            a_j = cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi, a_rec, phi_rec)\n",
        "        # Use quadratic interpolation otherwise\n",
        "        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):\n",
        "            qchk = delta2 * dalpha\n",
        "            a_j = quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
        "            if (a_j is None) or (a_j > b - qchk) or (a_j < a + qchk):\n",
        "                a_j = a_lo + 0.5 * dalpha\n",
        "\n",
        "        # Evaluate the function and its derivative at the new step size a_j\n",
        "        phi_aj = phi(a_j)\n",
        "        if (phi_aj > phi0 + c1 * a_j * derphi0) or (phi_aj >= phi_lo):\n",
        "            phi_rec, a_rec, a_hi, phi_hi = phi_hi, a_hi, a_j, phi_aj\n",
        "        else:\n",
        "            derphi_aj = derphi(a_j)\n",
        "            if abs(derphi_aj) <= -c2 * derphi0:\n",
        "                # Wolfe conditions are satisfied\n",
        "                return a_j, phi_aj, derphi_aj\n",
        "            if derphi_aj * (a_hi - a_lo) >= 0:\n",
        "                phi_rec, a_rec, a_hi, phi_hi = phi_hi, a_hi, a_lo, phi_lo\n",
        "            else:\n",
        "                phi_rec, a_rec = phi_lo, a_lo\n",
        "            a_lo, phi_lo, derphi_lo = a_j, phi_aj, derphi_aj\n",
        "        i += 1\n",
        "        if i > maxiter:\n",
        "            print('Failed to find a conforming step size')\n",
        "            return None, None, None\n",
        "\n",
        "# Wolfe conditions line search\n",
        "def line_search_wolfe(fn, fnp, xk, pk, c1 = 1e-4, c2 = 0.9, amax = np.Inf, maxiter = 10):\n",
        "    \"\"\"\n",
        "    Performs a line search to satisfy the Wolfe conditions.\n",
        "    \"\"\"\n",
        "    func_it = [0]\n",
        "    grad_it = [0]\n",
        "    grad_0 = [None]\n",
        "    grad_a = [None]\n",
        "\n",
        "    def phi(alpha):\n",
        "        # Function value for line search\n",
        "        func_it[0] += 1\n",
        "        return fn(xk + alpha*pk)\n",
        "\n",
        "    def phip(alpha):\n",
        "        # Derivative value for line search\n",
        "        grad_it[0] += 1\n",
        "        grad_0[0] = fnp(xk + alpha*pk)\n",
        "        grad_a[0] = alpha\n",
        "        return np.dot(grad_0[0], pk)\n",
        "\n",
        "    # Evaluate the function and its gradient at the initial point\n",
        "    phi_0 = phi(0)\n",
        "    phip_0 = phip(0)\n",
        "\n",
        "    # Initial guess for the step size\n",
        "    a0 = 0\n",
        "    a1 = 1.0\n",
        "\n",
        "    # Ensure that the step size does not exceed the maximum allowed value\n",
        "    if amax is not None:\n",
        "        a1 = min(a1, amax)\n",
        "\n",
        "    # Main loop for the line search\n",
        "    for i in range(maxiter):\n",
        "        # Evaluate the function at the new step size\n",
        "        phi_a1 = phi(a1)\n",
        "        phi_a0 = phi_0\n",
        "        phip_a0 = phip_0\n",
        "\n",
        "        not_first_iteration = i > 0\n",
        "        # Check if the Wolfe conditions are satisfied or need to zoom in\n",
        "        if (phi_a1 > phi_0 + c1 * a1 * phip_0) or \\\n",
        "               ((phi_a1 >= phi_a0) and not_first_iteration):\n",
        "            a_star, phi_star, phip_star = zoom(a0, a1, phi_a0,\n",
        "                                  phi_a1, phip_a0, phi, phip,\n",
        "                                  phi_0, phip_0, c1, c2)\n",
        "            break\n",
        "\n",
        "        phip_a1 = phip(a1)\n",
        "        if (abs(phip_a1) <= -c2*phip_0):\n",
        "          a_star = a1\n",
        "          phi_star = phi_a1\n",
        "          phip_star = phip_a1\n",
        "          break\n",
        "\n",
        "        if(phip_a1 >= 0):\n",
        "          a_star, phi_star, phip_star = zoom(a0, a1, phi_a0,\n",
        "                                  phi_a1, phip_a0, phi, phip,\n",
        "                                  phi_0, phip_0, c1, c2)\n",
        "          break\n",
        "\n",
        "        # Update the step size for the next iteration\n",
        "        a2 = 2 * a1\n",
        "        if amax is not None:\n",
        "          a2 = min(a2, amax)\n",
        "        a0 = a1\n",
        "        a1 = a2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi(a1)\n",
        "        phip_a0 = phip_a1\n",
        "    else:\n",
        "        # If the maximum number of iterations is reached without satisfying the conditions\n",
        "        a_star = a1\n",
        "        phi_star = phi_a1\n",
        "        phip_star = None\n",
        "        print('The line search algorithm did not converge')\n",
        "\n",
        "    if phip_star is None:\n",
        "        print('The line search algorithm did not converge')\n",
        "    else:\n",
        "        phip_star = grad_0[0]\n",
        "\n",
        "    return a_star\n",
        "\n",
        "# Steepest descent optimization algorithm\n",
        "def steepest_descent(fn, fnp, xk, maxiter=100000):\n",
        "    \"\"\"\n",
        "    Minimizes a function using the steepest descent method.\n",
        "    \"\"\"\n",
        "    k = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while k < maxiter:\n",
        "        gk = fnp(xk)  # Calculate the gradient at the current point\n",
        "\n",
        "        if np.linalg.norm(gk) <= 1e-3:\n",
        "            break  # Check for optimality\n",
        "\n",
        "        alpha = line_search_wolfe(fn, fnp, xk, -gk)  # Perform line search to find the step size\n",
        "\n",
        "        if alpha is None:\n",
        "            # Handle cases where the line search fails to find a step size\n",
        "            print(\"Failed to find a conforming step size. Using a small default step size.\")\n",
        "            alpha = 1e-4\n",
        "\n",
        "        xk = xk + alpha * (-gk)  # Update the current point using the step size\n",
        "        k += 1  # Increment iteration counter\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "\n",
        "    optimality_reached = np.linalg.norm(fnp(xk)) <= 1e-3  # Check if the solution is optimal\n",
        "\n",
        "    return xk, k, elapsed_time, optimality_reached\n",
        "\n",
        "# Suppress runtime warnings related to numerical issues during optimization\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    result_powell_bs1, iter_powell_bs1, time_powell_bs1, opt_powell_bs1 = steepest_descent(pbo, pbg, x0)\n",
        "    result_brown_den1, iter_brown_den1, time_brown_den1, opt_brown_den1 = steepest_descent(bdo, bdg, a1)\n",
        "    result_gulf1, iter_gulf1, time_gulf1, opt_gulf1 = steepest_descent(gulfo, gulfg, a2)\n",
        "\n",
        "# Output the results for the steepest descent algorithm\n",
        "print(\"Results for Steepest Descent algorithm\\n\")\n",
        "print(\"Result for powell_bs:\", result_powell_bs1)\n",
        "print(\"Number of iterations for powell_bs:\", iter_powell_bs1)\n",
        "print(\"Running time for powell_bs:\", time_powell_bs1)\n",
        "print(\"Optimality reached for powell_bs:\", opt_powell_bs1)\n",
        "\n",
        "print(\"\\nResult for brown_den:\", result_brown_den1)\n",
        "print(\"Number of iterations for brown_den:\", iter_brown_den1)\n",
        "print(\"Running time for brown_den:\", time_brown_den1)\n",
        "print(\"Optimality reached for brown_den:\", opt_brown_den1)\n",
        "\n",
        "print(\"\\nResult for gulf:\", result_gulf1)\n",
        "print(\"Number of iterations for gulf:\", iter_gulf1)\n",
        "print(\"Running time for gulf:\", time_gulf1)\n",
        "print(\"Optimality reached for gulf:\", opt_gulf1)\n"
      ],
      "metadata": {
        "id": "dzOdvdompteh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81944241-115b-4f48-9163-df699dcaf161"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Steepest Descent algorithm\n",
            "\n",
            "Result for powell_bs: [0.01004816 0.01004816]\n",
            "Number of iterations for powell_bs: 12\n",
            "Running time for powell_bs: 0.06866860389709473\n",
            "Optimality reached for powell_bs: True\n",
            "\n",
            "Result for brown_den: [-11.59443991  13.20363006  -0.40343971   0.23677947]\n",
            "Number of iterations for brown_den: 450\n",
            "Running time for brown_den: 5.332188367843628\n",
            "Optimality reached for brown_den: True\n",
            "\n",
            "Result for gulf: [35.02836788 25.67770834  1.39301742]\n",
            "Number of iterations for gulf: 3059\n",
            "Running time for gulf: 35.49288463592529\n",
            "Optimality reached for gulf: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DFP Algorithm implementation and results.**"
      ],
      "metadata": {
        "id": "g1vr8hTeHn2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimizing_function(f, grad_f, xk, dk, alpha_init=1, c1=1e-4, c2=0.9, max_iter=100):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - f: Objective function to minimize.\n",
        "    - grad_f: Gradient of the objective function.\n",
        "    - xk: Current point in the parameter space.\n",
        "    - dk: Current search direction.\n",
        "    - alpha_init: Initial guess for the step size.\n",
        "    - c1: Constant for the Wolfe condition related to sufficient decrease.\n",
        "    - c2: Constant for the Wolfe condition related to the curvature.\n",
        "    - max_iter: Maximum iterations for line search.\n",
        "\n",
        "    Returns:\n",
        "    - alpha: Step size that satisfies Wolfe conditions.\n",
        "    - i + 1: Number of iterations taken to find the step size.\n",
        "    \"\"\"\n",
        "    alpha = alpha_init\n",
        "    for i in range(max_iter):\n",
        "        # First Wolfe condition: Armijo rule for sufficient decrease\n",
        "        if f(xk + alpha * dk) > f(xk) + c1 * alpha * np.dot(grad_f(xk), dk):\n",
        "            alpha *= 0.5  # If not satisfied, reduce alpha\n",
        "        # Second Wolfe condition: Curvature condition\n",
        "        elif np.dot(grad_f(xk + alpha * dk), dk) < c2 * np.dot(grad_f(xk), dk):\n",
        "            alpha *= 1.1  # If not satisfied, increase alpha\n",
        "        else:\n",
        "            # If both conditions are satisfied, return the step size\n",
        "            return alpha, i + 1\n",
        "    # If Wolfe conditions are not satisfied within max_iter, raise an error\n",
        "    raise ValueError(\"Wolfe line search did not converge within the maximum number of iterations\")\n",
        "\n",
        "# Updated DFP method with Wolfe line search\n",
        "def minimize_dfp(f, grad_f, x0, max_iter=100, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Minimize a function using the Davidon-Fletcher-Powell (DFP) algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - f: Objective function to minimize.\n",
        "    - grad_f: Gradient of the objective function.\n",
        "    - x0: Initial guess for the parameters.\n",
        "    - max_iter: Maximum iterations for the DFP algorithm.\n",
        "    - epsilon: Tolerance for the stopping criterion based on the gradient norm.\n",
        "\n",
        "    Returns:\n",
        "    - xk: Estimated position of the minimum.\n",
        "    - iteration_count: Number of iterations taken to converge.\n",
        "    - running_time: Time taken to converge.\n",
        "    - optimality_reached: Boolean indicating if optimality was reached.\n",
        "    \"\"\"\n",
        "    n = len(x0)\n",
        "    xk = x0\n",
        "    Hk = np.eye(n)  # Initialize Hessian approximation to the identity matrix\n",
        "    iteration_count = 0  # Initialize iteration counter\n",
        "    optimality_reached = False  # Initialize optimality flag\n",
        "\n",
        "    start_time = time.time()  # Start timer\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        iteration_count += 1\n",
        "        gk = grad_f(xk)\n",
        "        if np.linalg.norm(gk) < epsilon:\n",
        "            optimality_reached = True  # Check if optimality condition is met\n",
        "            break\n",
        "\n",
        "        dk = -np.dot(Hk, gk)  # Calculate search direction\n",
        "        try:\n",
        "            step_size, iters = minimizing_function(f, grad_f, xk, dk)  # Wolfe line search\n",
        "        except ValueError as e:\n",
        "            print(str(e))\n",
        "            break\n",
        "\n",
        "        xk_new = xk + step_size * dk  # Update position\n",
        "        gk_new = grad_f(xk_new)\n",
        "        yk = gk_new - gk\n",
        "        sk = xk_new - xk\n",
        "        rho_k = 1.0 / np.dot(yk, sk)\n",
        "        # Update Hessian approximation using DFP formula\n",
        "        Hk_new = (np.eye(n) - rho_k * np.outer(sk, yk)) @ Hk @ (np.eye(n) - rho_k * np.outer(yk, sk)) + rho_k * np.outer(sk, sk)\n",
        "        xk = xk_new\n",
        "        Hk = Hk_new\n",
        "\n",
        "    end_time = time.time()  # End timer\n",
        "    running_time = end_time - start_time  # Calculate elapsed time\n",
        "\n",
        "    return xk, iteration_count, running_time, optimality_reached\n",
        "\n",
        "# Suppress warnings that may occur during the optimization process\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    result_powell_bs3 ,iter_powell_bs3, time_powell_bs3, opt_powell_bs3 = minimize_dfp(pbo, pbg, x0)\n",
        "    result_brown_den3, iter_brown_den3, time_brown_den3, opt_brown_den3 = minimize_dfp(bdo, bdg, a1)\n",
        "    result_gulf3, iter_gulf3, time_gulf3, opt_gulf3 = minimize_dfp(gulfo, gulfg, a2)\n",
        "\n",
        "# Print results for the DFP algorithm\n",
        "print(\"Results for DFP algorithm\\n\")\n",
        "print(\"Result for powell_bs:\", result_powell_bs3)\n",
        "print(\"Number of iterations for powell_bs:\", iter_powell_bs3)\n",
        "print(\"Running time for powell_bs:\", time_powell_bs3)\n",
        "print(\"Optimality reached for powell_bs:\", opt_powell_bs3)\n",
        "\n",
        "print(\"\\nResult for brown_den:\", result_brown_den3)\n",
        "print(\"Number of iterations for brown_den:\", iter_brown_den3)\n",
        "print(\"Running time for brown_den:\", time_brown_den3)\n",
        "print(\"Optimality reached for brown_den:\", opt_brown_den3)\n",
        "\n",
        "print(\"\\nResult for gulf:\", result_gulf3)\n",
        "print(\"Number of iterations for gulf:\", iter_gulf3)\n",
        "print(\"Running time for gulf:\", time_gulf3)\n",
        "print(\"Optimality reached for gulf:\", opt_gulf3)\n",
        "\n"
      ],
      "metadata": {
        "id": "lPlnYNfPwPkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb08266d-c95f-4a58-f2ba-a08ac2adb63d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for DFP algorithm\n",
            "\n",
            "Result for powell_bs: [-0.00994809 -0.00994809]\n",
            "Number of iterations for powell_bs: 54\n",
            "Running time for powell_bs: 0.04343056678771973\n",
            "Optimality reached for powell_bs: True\n",
            "\n",
            "Result for brown_den: [-11.5944399   13.20363005  -0.40343949   0.23677877]\n",
            "Number of iterations for brown_den: 26\n",
            "Running time for brown_den: 0.11340570449829102\n",
            "Optimality reached for brown_den: True\n",
            "\n",
            "Result for gulf: [50.00004358 24.99998541  1.49999971]\n",
            "Number of iterations for gulf: 31\n",
            "Running time for gulf: 0.3435060977935791\n",
            "Optimality reached for gulf: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BFGS implementation and results**"
      ],
      "metadata": {
        "id": "ihImSzchIHgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimize_bfgs(fn, fnp, xk, maxiter=100000):\n",
        "    \"\"\"\n",
        "    Minimize a function using the BFGS algorithm.\n",
        "\n",
        "    :param fn: The function to minimize.\n",
        "    :param fnp: The gradient of the function.\n",
        "    :param xk: Initial guess for the variables.\n",
        "    :param maxiter: Maximum number of iterations allowed.\n",
        "    :return: Tuple containing the optimal solution, number of iterations, elapsed time, and optimality flag.\n",
        "    \"\"\"\n",
        "    k = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize the inverse Hessian approximation as the identity matrix\n",
        "    n = len(xk)\n",
        "    Bk = np.eye(n)\n",
        "\n",
        "    while k < maxiter:\n",
        "        # Calculate the gradient at the current point\n",
        "        gk = fnp(xk)\n",
        "\n",
        "        # Termination condition: if the norm of the gradient is small enough\n",
        "        if np.linalg.norm(gk) <= 1e-3:\n",
        "            break\n",
        "\n",
        "        # Compute the search direction using the BFGS update\n",
        "        pk = -np.dot(Bk, gk)\n",
        "\n",
        "        # Perform a line search to find an acceptable step size that satisfies Wolfe conditions\n",
        "        alpha = line_search_wolfe(fn, fnp, xk, pk)\n",
        "\n",
        "        # If line search fails, use a default small step size\n",
        "        if alpha is None:\n",
        "            print(\"Failed to find a conforming step size. Using a small default step size.\")\n",
        "            alpha = 1e-4\n",
        "\n",
        "        # Update the position\n",
        "        xk_new = xk + alpha * pk\n",
        "\n",
        "        # Compute sk and yk vectors for BFGS update\n",
        "        sk = xk_new - xk\n",
        "        yk = fnp(xk_new) - gk\n",
        "        rho_k = 1.0 / np.dot(yk, sk)\n",
        "        A1 = np.eye(n) - rho_k * np.outer(sk, yk)\n",
        "        A2 = np.eye(n) - rho_k * np.outer(yk, sk)\n",
        "        Bk = np.dot(A1, np.dot(Bk, A2)) + rho_k * np.outer(sk, sk)\n",
        "\n",
        "        # Update the current estimate\n",
        "        xk = xk_new\n",
        "        k += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Check if the solution is optimal\n",
        "    optimality_reached = np.linalg.norm(fnp(xk)) <= 1e-3\n",
        "\n",
        "    return xk, k, elapsed_time, optimality_reached\n",
        "\n",
        "# Ignore runtime warnings that may occur during optimization\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    # Call minimize_bfgs function with the problem-specific functions and initial guesses\n",
        "    result_powell_bs2, iter_powell_bs2, time_powell_bs2, opt_powell_bs2 = minimize_bfgs(pbo, pbg, x0)\n",
        "    result_brown_den2, iter_brown_den2, time_brown_den2, opt_brown_den2 = minimize_bfgs(bdo, bdg, a1)\n",
        "    result_gulf2, iter_gulf2, time_gulf2, opt_gulf2 = minimize_bfgs(gulfo, gulfg, a2)\n",
        "\n",
        "# Output the results\n",
        "print(\"Results for BFGS algorithm\\n\")\n",
        "print(\"Result for powell_bs:\", result_powell_bs2)\n",
        "print(\"Number of iterations for powell_bs:\", iter_powell_bs2)\n",
        "print(\"Running time for powell_bs:\", time_powell_bs2)\n",
        "print(\"Optimality reached for powell_bs:\", opt_powell_bs2)\n",
        "\n",
        "print(\"\\nResult for brown_den:\", result_brown_den2)\n",
        "print(\"Number of iterations for brown_den:\", iter_brown_den2)\n",
        "print(\"Running time for brown_den:\", time_brown_den2)\n",
        "print(\"Optimality reached for brown_den:\", opt_brown_den2)\n",
        "\n",
        "print(\"\\nResult for gulf:\", result_gulf2)\n",
        "print(\"Number of iterations for gulf:\", iter_gulf2)\n",
        "print(\"Running time for gulf:\", time_gulf2)\n",
        "print(\"Optimality reached for gulf:\", opt_gulf2)\n"
      ],
      "metadata": {
        "id": "MXEIboNZ8nzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf154ed4-d229-4cb2-eaff-eebd7470ad55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for BFGS algorithm\n",
            "\n",
            "Result for powell_bs: [-0.00994847 -0.0099477 ]\n",
            "Number of iterations for powell_bs: 51\n",
            "Running time for powell_bs: 0.03192448616027832\n",
            "Optimality reached for powell_bs: True\n",
            "\n",
            "Result for brown_den: [-11.59443988  13.20363005  -0.4034395    0.23677872]\n",
            "Number of iterations for brown_den: 19\n",
            "Running time for brown_den: 0.06423521041870117\n",
            "Optimality reached for brown_den: True\n",
            "\n",
            "Result for gulf: [52.5403872  24.90590268  1.51471642]\n",
            "Number of iterations for gulf: 15\n",
            "Running time for gulf: 0.13026642799377441\n",
            "Optimality reached for gulf: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Here is table with results**"
      ],
      "metadata": {
        "id": "y3Q5LJa0JUnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = [\"Method\", \"Powell_BS Result\", \"Powell_BS Iterations\", \"Powell_BS Time\", \"Powell_BS Optimality\",\n",
        "                     \"Brown_Den Result\", \"Brown_Dennis Iterations\", \"Brown_Den Time\", \"Brown_Den Optimality\",\n",
        "                     \"Gulf Result\", \"Gulf Iterations\", \"Gulf Time\", \"Gulf Optimality\"]\n",
        "\n",
        "table.add_row([\"Nelder-Mead\", result_powell_bs, iter_powell_bs, time_powell_bs, opt_powell_bs,\n",
        "               result_brown_den, iter_brown_den, time_brown_den, opt_brown_den,\n",
        "               result_gulf, iter_gulf, time_gulf, opt_gulf])\n",
        "\n",
        "table.add_row([\"Steepest Descent\", result_powell_bs1, iter_powell_bs1, time_powell_bs1, opt_powell_bs1,\n",
        "               result_brown_den1, iter_brown_den1, time_brown_den1, opt_brown_den1,\n",
        "               result_gulf1, iter_gulf1, time_gulf1, opt_gulf1])\n",
        "\n",
        "table.add_row([\"DFP\", result_powell_bs3, iter_powell_bs3, time_powell_bs3, opt_powell_bs3,\n",
        "               result_brown_den3, iter_brown_den3, time_brown_den3, opt_brown_den3,\n",
        "               result_gulf3, iter_gulf3, time_gulf3, opt_gulf3])\n",
        "\n",
        "table.add_row([\"BFGS\", result_powell_bs2, iter_powell_bs2, time_powell_bs2, opt_powell_bs2,\n",
        "               result_brown_den2, iter_brown_den2, time_brown_den2, opt_brown_den2,\n",
        "               result_gulf2, iter_gulf2, time_gulf2, opt_gulf2])\n",
        "\n",
        "print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA-Xm0xYJdj-",
        "outputId": "07679d5e-5fb3-438a-f4ab-54f51c3357ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+---------------------------------+----------------------+---------------------+----------------------+-------------------------------------------------------+-------------------------+---------------------+----------------------+---------------------------------------+-----------------+---------------------+-----------------+\n",
            "|      Method      |         Powell_BS Result        | Powell_BS Iterations |    Powell_BS Time   | Powell_BS Optimality |                    Brown_Den Result                   | Brown_Dennis Iterations |    Brown_Den Time   | Brown_Den Optimality |              Gulf Result              | Gulf Iterations |      Gulf Time      | Gulf Optimality |\n",
            "+------------------+---------------------------------+----------------------+---------------------+----------------------+-------------------------------------------------------+-------------------------+---------------------+----------------------+---------------------------------------+-----------------+---------------------+-----------------+\n",
            "|   Nelder-Mead    | [9.10614509e+00 1.09815953e-05] |         509          |  1.2984683513641357 |         True         | [-11.59443956  13.20363     -0.40343998   0.23677878] |           214           |  1.726170539855957  |         True         | [50.00000249 24.99999994  1.50000002] |       274       |   7.42261815071106  |       True      |\n",
            "| Steepest Descent |     [0.01004816 0.01004816]     |          12          | 0.06866860389709473 |         True         | [-11.59443991  13.20363006  -0.40343971   0.23677947] |           450           |  5.332188367843628  |         True         | [35.02836788 25.67770834  1.39301742] |       3059      |  35.49288463592529  |       True      |\n",
            "|       DFP        |    [-0.00994809 -0.00994809]    |          54          | 0.04343056678771973 |         True         | [-11.5944399   13.20363005  -0.40343949   0.23677877] |            26           | 0.11340570449829102 |         True         | [50.00004358 24.99998541  1.49999971] |        31       |  0.3435060977935791 |       True      |\n",
            "|       BFGS       |    [-0.00994847 -0.0099477 ]    |          51          | 0.03192448616027832 |         True         | [-11.59443988  13.20363005  -0.4034395    0.23677872] |            19           | 0.06423521041870117 |         True         | [52.5403872  24.90590268  1.51471642] |        15       | 0.13026642799377441 |       True      |\n",
            "+------------------+---------------------------------+----------------------+---------------------+----------------------+-------------------------------------------------------+-------------------------+---------------------+----------------------+---------------------------------------+-----------------+---------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2**"
      ],
      "metadata": {
        "id": "tmXc_Td1XBoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST data using sklearn\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "# Preprocess the data\n",
        "# Select the first 100 images and labels from the dataset\n",
        "X = mnist.data.iloc[:100].values / 255.0  # Normalize\n",
        "y = mnist.target.iloc[:100].values.astype(int)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_one_hot = np.zeros((y.size, y.max()+1))\n",
        "y_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "# Print shapes to confirm\n",
        "X.shape, y_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjbnRxLeXKQu",
        "outputId": "315706c5-1a6d-4a2f-e577-ca9507301900"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 784), (100, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining neural network with its functions and using steepest descent to minimize entropy loss**\n",
        "\n",
        "# **Changes have been made to steepest descent algorithm for neural network**"
      ],
      "metadata": {
        "id": "JECRDF56VHbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0) * 1.0\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Neural Network Class\n",
        "class FullyConnectedNN:\n",
        "    def __init__(self):\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(784, 128) * 0.01\n",
        "        self.b1 = np.zeros((1, 128))\n",
        "        self.W2 = np.random.randn(128, 10) * 0.01\n",
        "        self.b2 = np.zeros((1, 10))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.A1 = relu(self.Z1)\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
        "        self.A2 = softmax(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        # Compute the cross-entropy loss\n",
        "        m = y_true.shape[0]\n",
        "        loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y_true):\n",
        "        # Backward pass\n",
        "        m = y_true.shape[0]\n",
        "        dZ2 = self.A2 - y_true\n",
        "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * relu_derivative(self.Z1)\n",
        "        dW1 = np.dot(X.T, dZ1) / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "        return gradients\n",
        "    def flatten_parameters(self):\n",
        "        return np.concatenate([self.W1.ravel(), self.b1.ravel(), self.W2.ravel(), self.b2.ravel()])\n",
        "\n",
        "    def set_parameters(self, flat_params):\n",
        "        w1_end = 784 * 128\n",
        "        b1_end = w1_end + 128\n",
        "        w2_end = b1_end + 128 * 10\n",
        "        self.W1 = flat_params[:w1_end].reshape(784, 128)\n",
        "        self.b1 = flat_params[w1_end:b1_end].reshape(1, 128)\n",
        "        self.W2 = flat_params[b1_end:w2_end].reshape(128, 10)\n",
        "        self.b2 = flat_params[w2_end:].reshape(1, 10)\n",
        "\n",
        "    def loss_and_grad(self, flat_params, X, y):\n",
        "        self.set_parameters(flat_params)\n",
        "        y_pred = self.forward(X)\n",
        "        loss = self.compute_loss(y_pred, y)\n",
        "        gradients = self.backward(X, y)\n",
        "        grad_flat = np.concatenate([gradients['dW1'].ravel(), gradients['db1'].ravel(),\n",
        "                                    gradients['dW2'].ravel(), gradients['db2'].ravel()])\n",
        "        return loss, grad_flat\n",
        "\n",
        "def steepest_descent(nn, X, y, maxiter=100, tol=1e-3):\n",
        "    flat_params = nn.flatten_parameters()\n",
        "\n",
        "    def fn(flat_params):\n",
        "        loss, _ = nn.loss_and_grad(flat_params, X, y)\n",
        "        return loss\n",
        "\n",
        "    def fnp(flat_params):\n",
        "        _, grad = nn.loss_and_grad(flat_params, X, y)\n",
        "        return grad\n",
        "\n",
        "    k = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while k < maxiter:\n",
        "        gk = fnp(flat_params)\n",
        "        if np.linalg.norm(gk) <= tol:\n",
        "            break  # Convergence check\n",
        "\n",
        "        alpha = line_search_wolfe(fn, fnp, flat_params, -gk)\n",
        "        if alpha is None:\n",
        "            print(\"Failed to find a conforming step size. Using a small default step size.\")\n",
        "            alpha = 1e-4\n",
        "\n",
        "        flat_params = flat_params + alpha * (-gk)\n",
        "        nn.set_parameters(flat_params)  # Update neural network parameters\n",
        "        k += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    optimality_reached = np.linalg.norm(fnp(flat_params)) <= tol\n",
        "\n",
        "    return flat_params, k, elapsed_time, optimality_reached\n",
        "\n",
        "nn = FullyConnectedNN()\n",
        "steepest_descent(nn, X, y_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mGEzWH9XQMn",
        "outputId": "3053636a-66b4-4d67-e329-0044edcfefc3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.02004413, -0.00427742,  0.00362237, ...,  0.13664968,\n",
              "        -0.23319778,  0.08141984]),\n",
              " 88,\n",
              " 3.4250850677490234,\n",
              " True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nelder meader**\n",
        "\n",
        "## **It is not possible for colab or normal computer environment to calculate nelder mead optimization on neural network.**\n",
        "## **fully connected network with a hidden layer of size 128 and an output layer of size 10 leads to a significant number of weights and biases to optimize. Nelder-Mead does not scale well with dimensionality due to its need to maintain a simplex with n + 1 vertices in a n-dimensional space.**\n"
      ],
      "metadata": {
        "id": "yhJeazk1VrpQ"
      }
    }
  ]
}